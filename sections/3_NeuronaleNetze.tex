\section{Neuronale Netze}
Ein Neuron ist ein Schwingkreis. Es hat n Eingänge und einen Ausgang. Die Werte der
Eingänge werden summiert. Wenn die Summe der Eingänge einen Schwellwert übertrifft, wechselt der Ausgang, das Neuron feuert.
Für die Schwellwertfunktion, wird entweder die Signum (0/1) oder die Sigmoid-Funktion welche ``weiche'' Kanten hat und ableitbar ist verwendet.
\begin{figure}[htb]
	\centering
	\input{tikz/sigmoid}
	\caption{Sigmoid Funktion mit a=1}
\end{figure}

\subsection{Übersicht}
\begin{table}[!h]
	\begin{tabularx}{\textwidth}{l | X}
		Typ & Merkmale \\
		\hline
		klassisches Perzeptron &
		\vspace{-2.5mm}
		\begin{itemize}
			\item Kann nur linear separierbare Probleme lösen
			\item Ein einzelnes Perzeptron-Neuron kann 14 Schnittebenen legen
		\end{itemize}
		\\
		Werbos Perzeptron &
		\vspace{-2.5mm}
		\begin{itemize}
			\item Kann auch linear nicht separierbare Probleme lösen
			\item Hat einen zusäzlichen Eingang (x·y)
			\item Entspricht eigentlich einem Netz aus klassischen Perzeptronen mit einer
				Versteckten Schicht
		\end{itemize}
		\\
		Holographisches Perzeptron &
		\vspace{-2.5mm}
		\begin{itemize}
			\item Verwendet Komplexe Zahlen
			\item Sehr grosse Speicherfähigkeit
			\item Grosse Verarbeitungsgeschwindigkeit
			\item Stochastische Resonanz (benötigt Rauschen im Input)
		\end{itemize}
		\\
		Kohonen-Netze &
		\vspace{-2.5mm}
		\begin{itemize}
			\item Topographisches Netz
			\item Geeignet für TSP-Problem
		\end{itemize}
		\\
		Hopfield &
		\vspace{-2.5mm}
		\begin{itemize}
			\item Signum-Funktion als Ausgabefunktion
			\item Lernt nicht bzw. unüberwachtes Lernen
			\item Nur eine Schicht, gleichzeitig Ein- und Ausgabe
			\item Jedes der binären Neuronen mit jedem verbunden
			\item Erkennt invertierte Muster als gleich
		\end{itemize}
	\end{tabularx}
	\caption{Vergleich zwischen verschiedenen Neuronalen Netzen}
	\label{tab:nnetze}
\end{table}

\subsection{Klassisches Neuron / Perzeptron}
Das Perzeptron ist das klassische mathematische Neuron. Es besteht aus:\\
\begin{tabularx}{\textwidth}{XX}
	\vspace{-1.5cm}
	\begin{itemize}
		\item Summierer
		\item Feuerschwelle
		\item Ausgabefunktion (Sigmoid)
	\end{itemize} &
	\input{tikz/perzeptron}
\end{tabularx}

Die Feuerschwelle wird benötigt um Beispielsweise boolsche Gatter zu
implementieren, was sonst unmöglich wäre.  Sie kann mit einem Zusätzlichen
Eingang der immer auf 1 gesetzt ist umgesetzt werden.  Das Gewicht dieses
Eingangs entspricht dann der Feuerschwelle.

\subsection{Netze elementarer Perzeptronen}
\begin{itemize}
	\item Keine versteckte Schicht erlaubt nur linear trennbare Probleme
	\item Eine versteckte Schicht kann zwei konvexe offene oder abgeschlossene Bereiche trennen
	\item Zwei versteckte Schichten können auch nicht-kontinuierliche und nicht zusammenhängende
		Arumentmengen unterteilen. Anzahl und Form hängt von der Anzahl Knoten ab.
\end{itemize}
\begin{figure}[h!]
	\centering
	\input{tikz/werbos}
	\caption{Werbos als Netz von Perzeptronen (XOR-Funktion)}
\end{figure}

\subsubsection{Backpropagation}
Fehlerzurückverfolgung bei der Rückwärts durchs Netz Zurückverfolgt wird,
welche Gewichte den grössten Einfluss auf Die Ausgabe haben.
Dies entpricht der Ableitung nach den Gewichten und ist somit auch ein
Gradientenabstieg \verweiskurz{2_Gradientenabstieg}

\subsection{Holographisches Perzeptron}
Bei diesen Neuronen bleiben die Phaseninformation im Gegensatz zu den
Perzeptronen erhalten.

\subsection{Kohonennetze}
Kohonennetze sind selbstorganisierend. Sie bestehen aus aneinandergereiten
Neuronen, mit variablen Abstand (Gummiband).

\subsubsection{Beispiel Travelling Salesman}
\begin{itemize}
	\item Neuronen zufällig platzieren
	\item Auswahl zufälliger Stadt
	\item Das am nächsten liegende Neuron nehmen und näher zu dieser Stadt rücken, die anderen Neuronen nachziehen
	\item Wiederholen bis alle Neuronen ihren Platz gefunden haben
\end{itemize}

\subsection{Hopfield}
Das Hopfield-Netz ist geeignet um Muster (Bilder) zu kennen. Dabei wird pro
Pixel ein binäres Neuron verwendet (-1,1).  Dazu erstellt man eine
Korrelationsmatrix, welche alle Muster enthält, es wird nicht gelernt!  Für eine
gute Erkennung sollten die Muster möglichst orthogonal, also linear unabhängig
sein.  Die Berechnung der Korrellationsmatrix funktioniert folgendermassen:

\begin{itemize}
	\item Alle Muster definieren\\ $x_1 = \begin{bmatrix}-1&1&1&1 \end{bmatrix};
			x_2 = \begin{bmatrix}1&1&1&-1\end{bmatrix}$
	\item Korrelationsmatrizen erstellen:\\ $M_1 = x_1^T x_1 =
		\begin{bmatrix}
			 1&-1&-1&-1 \\
			-1& 1& 1& 1 \\
			-1& 1& 1& 1 \\
			-1& 1& 1& 1
		\end{bmatrix} M_2 =x_2^T x_2 =
		\begin{bmatrix}
			 1& 1& 1&-1\\
			 1& 1& 1&-1\\
			 1& 1& 1&-1\\
			-1&-1&-1& 1
		\end{bmatrix}$
	\item Die Energiematrix entspricht der Summe der Korrelatiosmatrizen:\\
		$M = M_1 + M_2 = 
		\begin{bmatrix}
			 2&0&0&-2\\
			 0&2&2& 0\\
			 0&2&2& 0\\
			-2&0&0& 2
		\end{bmatrix}$
	\item Diagonalelemente auf 0 setzen: $M_{i,i} = 0 \rightarrow
		M=\begin{bmatrix}
			 0&0&0&-2\\
			 0&0&2& 0\\
			 0&2&0& 0\\
			-2&0&0& 0
		\end{bmatrix}$
\end{itemize}
\subsubsection{Benötige Anzahl Neuronen}
Pro zu erkennendes Muster ($M$) werden etwa 7 Neuronen ($N$) benötigt.
\begin{itemize}
	\item Bis einige 100 Neuronen: $M_{max} ~ 0.15N$
	\item Für Netze mit sehr grossem N: $M_{max} ~ \frac{0.5N}{log(N)}$
	\item Allgemein gilt $M_{max} < N$
	\item Für $M >= 0.15N$ bilden sich mehr Nebenminimas in denen das Netz
		hängen bleiben kann (Rütteln kann helfen hinauszukommen)
	\item Ab $M > 0.5N$ degenerieren die Energiebarrieren zwischen den Minima,
		wodruch das Netz unbrauchbar wird
\end{itemize}

% vim:tw=80
